# 遇到困难
## 在优化代码结构后，用浏览器对服务器发起请求时，服务器处于阻塞状态

### 分析原因
1. 为了减小主函数代码量，主函数都用函数调用完成相应功能，一开始认为失败原因可能是形参传递问题，也就是在函数中并未修改实际的值；
2. 使用GDB调试，发现工作线程一直会在互斥锁的地方阻塞，定位到问题所在

### 解决困难
```
	//互斥锁+条件变量
	while(!m_stop)  //while(m_stop==false)
	{
		pthread_mutex_lock(&lock);
		if(m_workqueue.empty())
		{
			pthread_cond_wait(&notify, &lock);
			//continue;
		}
		http_conn* request = m_workqueue.front(); 
		m_workqueue.pop_front(); 
		pthread_mutex_unlock(&lock);
		if(!request)
			continue;
		request->process();
	}

	//互斥锁+信号量
	while(!m_stop)  //while(m_stop==false)
	{
		m_queuestat.wait(); //将信号量减1，如果信号量的值为0，则它会阻塞，一开始所有创建的线程都阻塞在这，直到post
		m_queuelocker.lock();
		if(m_workqueue.empty())
		{
			m_queuelocker.unlock();
			continue;
		}
		T* request = m_workqueue.front(); 
		m_workqueue.pop_front(); 
		m_queuelocker.unlock();
		if(!request)
			continue;
		request->process();
	}
```
如上代码所示，我将信号量改为条件变量后，互斥锁+条件变量与原来互斥锁+信号量的逻辑有所不同，而这里在判断请求队列为空时，条件变量wait，当往请求队列加入请求后，条件变量会signal，这里的工作线线程会往下执行，但是下一句是contiue，又跳出了循环，导致再次上锁，无法往下执行，所以这里不需要加contiue（之前改代码忘记删了）
### 收获
引起这个bug的原因很简单，但是发现bug却花了很多时间，这里主要收获就是如何利用GDB调试多线程程序。
简单总结：
1. 编译时加上-g就可以用gdb对可执行文件进行调试
2. 对file.cpp的第n行下断点
```
b file.cpp:n
```
3. 运行可执行文件（后可加argc的参数）
```
r 127.0.0.1 8888
```
4. 单步执行n，继续往下执行c
5. 显示当前可调式的所有线程：info threads
6. 切换调试ID线程：thread ID
7. 调试多线程时默认被调试的线程外，其他线程也会继续执行，可以通过以下命令控制其他线程（off表示以默认方式，on表示只有被调试的线程执行，step表示在单步执行时只有当前线程会执行）
``` 
set scheduler-locking [off|on|step]
```

## 在压力测试过程中，短链接一直测试失败
![short](https://github.com/ofdm/myHttpServer/blob/master/src/test_img/fail.png)

### 分析原因
1. 开始以为代码问题，认为没有解析Connection:close，以为遇到close时认为是错误而直接返回，但是通过在关键位置设置打印发现没有问题
2. 接着从git上下载了别人写的服务器代码测试，发现别人的测试结果没问题，然后考虑是否是在请求完成后关闭处的代码问题，但是对比之后也没有发现问题
3. 使用了netstat工具查看状态发现别人在短连接测试时，关闭的客户时进入了TIME_WAIT状态，而我的是直接不存在了，发现不同后，找到了问题所在
![netstat](https://github.com/ofdm/myHttpServer/blob/master/src/test_img/连接状态.png)

### 解决困难
发现自己在一开始设置了SO_LINGER的socket选项
```
/*
#include <sys/socket.h>
struct linger
{
	int l_onoff;
	int l_linger;
}
*/
struct linger tmp = {1, 0};
setsocket(socket, SOL_SOCKET, SO_LINGER, &tmp, sizeof(tmp));
```
SO_LINGER选项是控制close系统调用在关闭TCP连接时的行为，默认情况下，当使用close系统调用关闭一个socket时，close立即返回，TCP模块负责把该socket对应的TCP发送缓冲区残留的数据发送给对方，所以会进入四次挥手的TIME_WAIT状态，
1. l_onoff=0，SO——LINGER不起作用；
2. l_onoff!=0，l_linger=0,close立即返回，TCP模块将丢弃被关闭的socket对应的TCP发送缓冲区的数据，同时发送对方一个复位报文段；
3. l_onoff!=0，l_linger>0,此时close的行为取决于两个条件：一是被关闭的socket对应的TCP发送缓冲区是否还有残留数据；二是该socket是否阻塞。对于阻塞的socket，close会等待l_linger时间，直到发送完数据，如果到时间了还没发送完，则close返回-1并设置erron为EWOULDBLOCK。若socket非阻塞，close立即返回，此时根据返回值和erron来判断是否发送完毕。
由于我这里设置成了第二种所以测试结果一直失败，直接注释该语句或者将struct linger tmp = {0, 0}解决问题。

### 收获
* 学到了一些Linux系统监测工具，如strace、netstat等
* 锻炼了寻找bug、解决bug的能力
* 重新整理了一遍整个代码思路